{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np \n",
    "from sklearn.ensemble import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decistion Trees \n",
    "\n",
    "- Partition the input space such that the gini index(classification) or the squared/huber/... loss (regression) are minimised "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Ensmble Methods \n",
    "**The goal of ensemble methods is to combine the predictions of several base estimators built with a given learning algorithm in order to improve generalizability / robustness over a single estimator**\n",
    "\n",
    "\n",
    "# Averaging Method\n",
    "The process is built around reducing the variance, whereas the base estimaotr naturally has lower bias \n",
    "\n",
    "the driving principle is to build several estimators independently and then to average their predictions. On average, the combined estimator is usually better than any of the single base estimator because its variance is reduced. \n",
    "\n",
    "## Bootstrap Aggregated Trees \n",
    "\n",
    "Training: \n",
    "\n",
    "- For dataset with $1,2,.... N$\n",
    "- Sample with replacement to create B dataset $D_1, D_2, .... D_B$\n",
    "- For each dataset, build a classifier/regressor tree, without any puring: $M_1, M_2, ... M_(B-1), M_B$\n",
    "\n",
    "\n",
    "Inference: \n",
    "- New datapoint coming in: $x$ \n",
    "- Take $MEAN[M_1, M_2, ... M_(B-1), M_B]$ on regression or $MAJORITY[M_1, M_2, ... M_(B-1), M_B]$ on classification \n",
    "\n",
    "\n",
    "NOTE: \n",
    "\n",
    "- When N --> infinite, for each data point $i$, it WON'T be included in B/3 datasets approximately \n",
    "- We can use those trees that do not trained on sample i to obtain the so called, Out-of-bag error during training, as our test error \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest / Decorelated Bootstrap Aggregated Tree \n",
    "\n",
    "Training: \n",
    "\n",
    "- For dataset with $1,2,.... N$\n",
    "- Sample with replacement to create B dataset $D_1, D_2, .... D_B$\n",
    "- For each dataset, build a classifier/regressor tree, \n",
    "    - without any puring: $M_1, M_2, ... M_(B-1), M_B$\n",
    "    - For each single time we split, only consider $m<=p$ features, $p$ being the number of features in $X$ \n",
    "    - m is a hyper-paremeter we can search using the Bayesian process\n",
    "        - m for classification = $sqrt(p)$\n",
    "        - m for regression = $int(p/3)$\n",
    "\n",
    "\n",
    "Inference: \n",
    "- New datapoint coming in: $x$ \n",
    "- Take $MEAN[M_1, M_2, ... M_(B-1), M_B]$ on regression or $MAJORITY[M_1, M_2, ... M_(B-1), M_B]$ on classification \n",
    "\n",
    "\n",
    "NOTE: \n",
    "\n",
    "- When N --> infinite, for each data point $i$, it WON'T be included in B/3 datasets approximately \n",
    "- We can use those trees that do not trained on sample i to obtain the so called, Out-of-bag error during training, as our test error "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Boosting method \n",
    "\n",
    "The process focusing on reducing the bias, the base model has low variance \n",
    "\n",
    "base estimators are built sequentially and one tries to reduce the bias of the combined estimator. The motivation is to combine several weak models to produce a powerful ensemble.\n",
    "\n",
    "## Gradient Boosting \n",
    "\n",
    "- ### XGBoost \n",
    "\n",
    "\n",
    "- ### Light Gradient Boosted Machine (LGBM)\n",
    "\n",
    "\n",
    "- ### CatBoost "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.9 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
